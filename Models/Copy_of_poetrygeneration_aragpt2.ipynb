{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bSA151gW7LQ8",
        "outputId": "5f751e0f-d9dc-4f34-f343-eece4e2be851"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing collected packages: pyyaml, tokenizers, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.7.0 pyyaml-6.0 tokenizers-0.12.1 transformers-4.19.2\n"
          ]
        }
      ],
      "source": [
        "pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Ul5LB2AqxjX9"
      },
      "outputs": [],
      "source": [
        "from transformers import GPT2TokenizerFast,GPT2LMHeadModel\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5DFO9YN-3SnB",
        "outputId": "3003c479-d567-4dd1-9424-81335e1b0cf7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "aZQPN8WqTtU3"
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv('/content/drive/MyDrive/Barmagan/chosen_data.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "QY7M2gzoxETJ"
      },
      "outputs": [],
      "source": [
        "model_name = 'aubmindlab/aragpt2-medium'\n",
        "\n",
        "tokenizer = GPT2TokenizerFast.from_pretrained(model_name)\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2kiW-UecxETN"
      },
      "source": [
        "**Tokenization**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "poems = list(data.new_poems2.values)"
      ],
      "metadata": {
        "id": "FOVbaYL8XOQV"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3JNNOh7TxETP",
        "outputId": "e49b4421-d4b7-4a72-9085-09b27152f76b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Embedding(64002, 1024)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "## add padding token to tokenizer, then modify model's vocab size\n",
        "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
        "tokenizer.add_special_tokens({'additional_special_tokens': ['[NEWLINE]']})\n",
        "model.resize_token_embeddings(len(tokenizer))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## treat first line as input\n",
        "def CreateInputs(poems, tokenizer):\n",
        "    input_lines = []\n",
        "    for poem in poems:\n",
        "        poem_lines = poem.split('\\n')\n",
        "        input_line = tokenizer.bos_token \n",
        "        for idx in range(0, len(poem_lines)-2):\n",
        "          \n",
        "          input_line += poem_lines[idx]+  ' ' + tokenizer.additional_special_tokens[0]\n",
        "        \n",
        "        input_line +=  poem_lines[3] + tokenizer.eos_token\n",
        "                \n",
        "        input_lines.append(input_line)\n",
        "            \n",
        "    return input_lines\n",
        "\n",
        "input_sentences = CreateInputs(poems, tokenizer)"
      ],
      "metadata": {
        "id": "ivg3x5A2Xn8E"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "bRCQtlMVxETP"
      },
      "outputs": [],
      "source": [
        "def get_max_length(input_sentences):\n",
        "    max_len = 0\n",
        "    for line in input_sentences:\n",
        "        encoded_line = tokenizer.encode(line)\n",
        "        encoded_len = len(encoded_line)\n",
        "        max_len = max(max_len, encoded_len)\n",
        "    \n",
        "    return max_len\n",
        " \n",
        "max_length = get_max_length(input_sentences)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def TokenizeAndEncodeInput(input_sentences, tokenizer, max_len):\n",
        "\n",
        "    encoded_input = []\n",
        "    attention_mask = []\n",
        " \n",
        "    for poem in input_sentences:\n",
        "\n",
        "            \n",
        "        encoded_line = tokenizer.encode_plus(poem,\n",
        "                                        max_length=max_len,\n",
        "                                        padding= 'max_length',\n",
        "                                        return_tensors= 'pt',\n",
        "                                            truncation= True)\n",
        "\n",
        "\n",
        "\n",
        "        encoded_input.append(encoded_line['input_ids'])\n",
        "        attention_mask.append(encoded_line['attention_mask'])\n",
        "\n",
        "              \n",
        "    \n",
        "    input_tensor = torch.cat(encoded_input, dim=0)\n",
        "    attention_tensor =torch.cat(attention_mask, dim=0)\n",
        "    \n",
        "        \n",
        "    return input_tensor, attention_tensor\n",
        "\n",
        "input_tensor, input_mask_tensor = TokenizeAndEncodeInput(input_sentences, tokenizer, max_length)"
      ],
      "metadata": {
        "id": "ttsu9a2A1-vY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cXJUWzTqxETQ",
        "outputId": "18a5ae44-7668-42b1-e80d-c7b2d1ad9e96"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([10000, 50])"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "input_mask_tensor.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "cR_vZCn-xETR"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
        "from transformers import AdamW, get_linear_schedule_with_warmup\n",
        "from tqdm import tqdm, trange\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "k3_mJ1nnxETS"
      },
      "outputs": [],
      "source": [
        "def create_dataloader(poetry_in, attention_mask,  batch_size):\n",
        "    \n",
        "    tensor_dataset = TensorDataset(poetry_in, attention_mask)\n",
        "\n",
        "    \n",
        "    train_dataloader = DataLoader(tensor_dataset, batch_size = batch_size)\n",
        "\n",
        "    return train_dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "1rtTKnjexETS"
      },
      "outputs": [],
      "source": [
        "def initiate(lr, warmup,total_steps, epochs, model = model):\n",
        "\n",
        "    \n",
        "    optimizer = AdamW(model.parameters(), lr=lr)\n",
        "    scheduler = get_linear_schedule_with_warmup(optimizer,\n",
        "                                                num_warmup_steps=warmup,\n",
        "                                                num_training_steps=total_steps)\n",
        "    \n",
        "    return optimizer, scheduler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "OvZmSc_yxETS"
      },
      "outputs": [],
      "source": [
        "if torch.cuda.is_available():\n",
        "    device = torch.device('cuda')\n",
        "else:\n",
        "    print('No GPU available, using CPU')\n",
        "    device = torch.device('cpu')\n",
        "        \n",
        "model.cuda()  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TOdpU5HnxETT"
      },
      "outputs": [],
      "source": [
        "epochs = 5\n",
        "train_dataloader = create_dataloader(input_tensor, input_mask_tensor,batch_size = 8)  \n",
        "total_steps = len(train_dataloader) * epochs\n",
        "warmup_steps = total_steps * 0.2\n",
        "   \n",
        "optimizer, scheduler = initiate(lr= 3e-5, epochs= epochs,warmup = warmup_steps,\n",
        "                                total_steps=total_steps)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G0f2Q-evxETT",
        "outputId": "7f51a910-a722-43ee-9260-e46431d62766"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 0: 100%|██████████| 1250/1250 [05:15<00:00,  3.97it/s, loss=3.29]\n",
            "Epoch 1: 100%|██████████| 1250/1250 [05:13<00:00,  3.98it/s, loss=2.78]\n",
            "Epoch 2: 100%|██████████| 1250/1250 [05:13<00:00,  3.98it/s, loss=1.9]\n",
            "Epoch 3: 100%|██████████| 1250/1250 [05:13<00:00,  3.99it/s, loss=1.18]\n",
            "Epoch 4: 100%|██████████| 1250/1250 [05:13<00:00,  3.98it/s, loss=0.722]\n"
          ]
        }
      ],
      "source": [
        "def train(lr, epochs, train_dataloader, model = model, tokenizer = tokenizer):\n",
        "    idx = 0\n",
        "    \n",
        "    for epoch in range(0, epochs):\n",
        "      loop = tqdm(train_dataloader, leave= True)\n",
        "      for batch in loop:\n",
        "        optimizer.zero_grad()\n",
        "        input_tensor  = batch[0].to(device)\n",
        "        attention_mask = batch[1].to(device)\n",
        "            \n",
        "        outputs = model(input_tensor, attention_mask = attention_mask, labels = input_tensor)\n",
        "        loss= outputs[0]\n",
        "        loss.backward()\n",
        "            \n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "        if(idx == 6000):\n",
        "          try:\n",
        "            model_name = 'poetry_generator'+str(epoch)\n",
        "            torch.save({\n",
        "                  'model_state_dict': model.state_dict(),\n",
        "                  'loss': loss,\n",
        "                  'optimizer_state_dict': optimizer.state_dict()\n",
        "              }, '/content/drive/MyDrive/Barmagan/' +model_name+'.pth')\n",
        "          except:\n",
        "            print('Something Went Wrong!')\n",
        "          \n",
        "        loop.set_description(f'Epoch {epoch}')\n",
        "        loop.set_postfix(loss = loss.item())\n",
        "            \n",
        "\n",
        "train(train_dataloader = train_dataloader, epochs = 5, lr =3e-5)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "model =GPT2LMHeadModel.from_pretrained('/content/drive/MyDrive/Barmagan/poetry_generator')\n"
      ],
      "metadata": {
        "id": "5QqTzzTD5IKg"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = tokenizer.bos_token + 'أبينا أبينا أن تضب لثاتكم' +tokenizer.eos_token"
      ],
      "metadata": {
        "id": "A5gtsCxi6V65"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "4Ws4UhfRxETU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "10bbfde7-d247-452a-e023-8f78b3ef3070"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "encoded_prompt = tokenizer.encode(prompt, return_tensors = 'pt')\n",
        "gneration = model.generate(encoded_prompt, top_k = 70, max_length =80, top_p= 0.2,\n",
        "                           repetition_penalty = 3.0,\n",
        "    no_repeat_ngram_size = 3, num_beams=15)\n",
        "                   \n",
        "generated_text = tokenizer.decode(gneration[0], skip_special_tokens = True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "1rRZ2MyAxETU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f9d715d-f331-472b-c9b3-f6666bb258fb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "أبينا أبينا أن تضب لثاتكم خصم ذوي مرديات إذا ما ضن بعض الحق بالباطل عن حوزتهم فإن الحق مظنون ومعروف مغيار لا يحلون حقكم في إمامكم ضرارا للسان وللدهر إحلاء وإمرار\n"
          ]
        }
      ],
      "source": [
        "print(generated_text)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Copy of poetrygeneration-aragpt2.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.12"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
